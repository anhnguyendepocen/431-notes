# Summarizing Categorical Variables

Summarizing categorical variables numerically is mostly about building tables, and calculating percentages or proportions. We'll save our discussion of modeling categorical data for later. Recall that in the `nh_adults` data set we built in Section \@ref(newNHANES) we had the following categorical variables. The number of levels indicates the number of possible categories for each categorical variable.

Variable     | Description                  | Levels | Type
----------:  | ---------------------------- | - | ---------
Sex          | sex of subject               | 2 | binary
Race         | subject's race               | 6 | nominal
Education    | subject's educational level  | 5 | ordinal
PhysActive   | Participates in sports?      | 2 | binary
Smoke100     | Smoked 100+ cigarettes?      | 2 | binary
SleepTrouble | Trouble sleeping?            | 2 | binary
HealthGen    | Self-report health           | 5 | ordinal

## The `summary` function for Categorical data

When R recognizes a variable as categorical, it stores it as a *factor*. Such variables get special treatment from the `summary` function, in particular a table of available values (so long as there aren't too many.)

```{r nh-adultscategorical-summ1}
nh_adults %>%
  select(Sex, Race, Education, PhysActive, Smoke100, 
         SleepTrouble, HealthGen, MaritalStatus) %>%
  summary()
```

## Tables to describe One Categorical Variable

Suppose we build a table (using the `tabyl` function from the `janitor` package) to describe the `HealthGen` distribution.

```{r nh_adults_HealthGen_counts}
nh_adults %>%
    tabyl(HealthGen) %>%
    adorn_pct_formatting()
```

Note how the missing (`<NA>`) values are not included in the `valid_percent` calculation, but are in the `percent` calculation. Note also the use of percentage formatting. 

What if we want to add a total count, sometimes called the *marginal* total?

```{r nh_adults_HealthGen_countsandmargins}
nh_adults %>%
    tabyl(HealthGen) %>%
    adorn_totals() %>%
    adorn_pct_formatting()
```

What about marital status, which has no missing data in our sample?

```{r nh_adults_MaritalStatus_countsandmargins}
nh_adults %>%
    tabyl(MaritalStatus) %>%
    adorn_totals() %>%
    adorn_pct_formatting()
```

## The Mode of a Categorical Variable

A common measure applied to a categorical variable is to identify the mode, the most frequently observed value. To find the mode for variables with lots of categories (so that the `summary` may not be sufficient), we usually tabulate the data, and then sort by the counts of the numbers of observations, as we did with discrete quantitative variables.

```{r mode_nh_adults_HealthGen}
nh_adults %>%
    group_by(HealthGen) %>%
    summarise(count = n()) %>%
    arrange(desc(count)) 
```

## `describe` in the `Hmisc` package

```{r Hmisc-describe-nh_adults_categorical}
Hmisc::describe(nh_adults %>% 
                    select(Sex, Race, Education, PhysActive, 
                           Smoke100, SleepTrouble, 
                           HealthGen, MaritalStatus))
```

## Cross-Tabulations 

It is very common for us to want to describe the association of one categorical variable with another. For instance, is there a relationship between Education and SleepTrouble in these data?

```{r crosstab1-nh_adults_education_sleep}
nh_adults %>%
    tabyl(Education, SleepTrouble) %>%
    adorn_totals(where = c("row", "col")) 
```

Note the use of `adorn_totals` to get the marginal counts, and how we specify that we want both the row and column totals. We can add a title for the columns with...

```{r crosstab1-nh_adults_education_sleep_titled}
nh_adults %>%
    tabyl(Education, SleepTrouble) %>%
    adorn_totals(where = c("row", "col")) %>%
    adorn_title(placement = "combined")
```

Often, we'll want to show percentages in a cross-tabulation like this. To get row percentages so that we can directly see the probability of `SleepTrouble = Yes` for each level of `Education`, we can use:

```{r crosstab1-nh_adults_education_sleep2}
nh_adults %>%
    tabyl(Education, SleepTrouble) %>%
    adorn_totals(where = "row") %>%
    adorn_percentages(denominator = "row") %>%
    adorn_pct_formatting() %>%
    adorn_title(placement = "combined")
```

If we want to compare the distribution of `Education` between the two levels of `SleepTrouble` with column percentages, we can use the following...

```{r crosstab1-nh_adults_education_sleep3}
nh_adults %>%
    tabyl(Education, SleepTrouble) %>%
    adorn_totals(where = "col") %>%
    adorn_percentages(denominator = "col") %>%
    adorn_pct_formatting() %>%
    adorn_title(placement = "combined") 
```

If we want overall percentages in the cells of the table, so that the total across all combinations of `Education` and `SleepTrouble` is 100%, we can use:

```{r crosstab1-nh_adults_education_sleep4}
nh_adults %>%
    tabyl(Education, SleepTrouble) %>%
    adorn_totals(where = c("row", "col")) %>%
    adorn_percentages(denominator = "all") %>%
    adorn_pct_formatting() %>%
    adorn_title(placement = "combined") 
```

Another common approach is to include both counts and percentages in a cross-tabulation. Let's look at the breakdown of `HealthGen` by `MaritalStatus`.

```{r}
nh_adults %>%
    tabyl(MaritalStatus, HealthGen) %>%
    adorn_totals(where = c("row")) %>%
    adorn_percentages(denominator = "row") %>%
    adorn_pct_formatting() %>%
    adorn_ns(position = "front") %>%
    adorn_title(placement = "combined") %>%
    knitr::kable()
```

What if we wanted to ignore the missing `HealthGen` values? Most often, I filter down to the complete observations.

```{r}
nh_adults %>%
    filter(complete.cases(MaritalStatus, HealthGen)) %>%
    tabyl(MaritalStatus, HealthGen) %>%
    adorn_totals(where = c("row")) %>%
    adorn_percentages(denominator = "row") %>%
    adorn_pct_formatting() %>%
    adorn_ns(position = "front") %>%
    adorn_title(placement = "combined")
```

For more on working with `tabyls`, see the vignette in the `janitor` package. There you'll find a complete list of all of the `adorn` functions, for example.

Here's another approach, to look at the cross-classification of Race and HealthGen:

```{r crosstab2-nh_adults_race_healthgen}
xtabs(~ Race + HealthGen, data = nh_adults)
```

### Cross-Classifying Three Categorical Variables

Suppose we are interested in `Smoke100` and its relationship to `PhysActive` and `SleepTrouble`.

```{r}
nh_adults %>%
    tabyl(Smoke100, PhysActive, SleepTrouble) %>%
    adorn_title(placement = "top")
```

The result here is a tabyl of `Smoke100` (rows) by `PhysActive` (columns), split into a list by `SleepTrouble`. Another approach to get the same table is:


```{r nh_adults_crosstab-3ways1}
xtabs(~ Smoke100 + PhysActive + SleepTrouble, data = nh_adults)
```

We can also build a **flat** version of this table, as follows:

```{r nh_adults_crosstab-3ways2}
ftable(Smoke100 ~ PhysActive + SleepTrouble, data = nh_adults)
```


And we can do this with `dplyr` functions, as well, for example...

```{r nh_adults_crosstab-3ways3}
nh_adults %>%
    select(Smoke100, PhysActive, SleepTrouble) %>%
    table() 
```

## Constructing Tables Well

The prolific Howard Wainer is responsible for many interesting books on visualization and related issues, including @HW_GraphicDiscovery and @HW_MedicalIlluminations. These rules come from Chapter 10 of @HW_VisualRevelations.

1. Order the rows and columns in a way that makes sense.
2. Round, a lot!
3. ALL is different and important

### Alabama First!

Which of these Tables is more useful to you?
    
2013 Percent of Students in grades 9-12 who are obese

State | % Obese | 95% CI | Sample Size
----- | ------- | ------ | -----------
Alabama | 17.1 | (14.6 - 19.9) | 1,499
Alaska |	12.4	| (10.5-14.6)	| 1,167
Arizona |	10.7 |	(8.3-13.6)	| 1,520
Arkansas |	17.8	| (15.7-20.1)	| 1,470
Connecticut |	12.3 |	(10.2-14.7)	| 2,270
Delaware |	14.2 |	(12.9-15.6) |	2,475
Florida |	11.6	| (10.5-12.8)	| 5,491
... | | | 
Wisconsin |	11.6 | 	(9.7-13.9)	| 2,771
Wyoming	| 10.7 |	(9.4-12.2)	| 2,910

or ...

State | % Obese | 95% CI | Sample Size
----- | ------- | ------ | -----------
Kentucky | 18.0 | (15.7 - 20.6) | 1,537
Arkansas | 17.8 | (15.7 - 20.1) | 1,470
Alabama | 17.1 | (14.6 - 19.9) | 1,499
Tennessee | 16.9 | (15.1 - 18.8) | 1,831
Texas | 15.7 | (13.9 - 17.6) | 3,039
... | | |
Massachusetts | 10.2 | (8.5 - 12.1) | 2,547
Idaho | 9.6 | (8.2 - 11.1) | 1,841
Montana | 9.4 | (8.4 - 10.5) | 4,679
New Jersey | 8.7 | (6.8 - 11.2) | 1,644
Utah | 6.4 | (4.8 - 8.5) | 2,136

It is a rare event when Alabama first is the best choice.

### Order rows and columns sensibly

- Alabama First!
    + Size places - put the largest first. We often look most carefully at the top.
- Order time from the past to the future to help the viewer.
- If there is a clear predictor-outcome relationship, put the predictors in the rows and the outcomes in the columns.

### Round - a lot!

- Humans cannot understand more than two digits very easily.
- We almost never care about accuracy of more than two digits.
- We can almost never justify more than two digits of accuracy statistically.
- It's also helpful to remember that we are almost invariably publishing progress to date, rather than a truly final answer.

Suppose, for instance, we report a correlation coefficient of 0.25. How many observations do you think you would need to justify such a choice?

- To report 0.25 meaningfully, we want to be sure that the second digit isn't 4 or 6.
- That requires a standard error less than 0.005
- The *standard error* of any statistic is proportional to 1 over the square root of the sample size, *n*.

So $\frac{1}{\sqrt{n}}$ ~ 0.005, but that means $\sqrt{n} = \frac{1}{0.005} = 200$. If $\sqrt{n} = 200$, then *n* = (200)^2^ = 40,000. 

Do we usually have 40,000 observations?

### ALL is different and important

Summaries of rows and columns provide a measure of what is typical or usual. Sometimes a sum is helpful, at other times, consider presenting a median or other summary. The ALL category, as @HW_VisualRevelations suggests, should be both visually different from the individual entries and set spatially apart.

On the whole, it's *far* easier to fall into a good graph in R (at least if you have some ggplot2 skills) than to produce a good table.
